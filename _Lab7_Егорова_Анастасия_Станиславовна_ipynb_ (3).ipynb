{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **MPI**"
      ],
      "metadata": {
        "id": "EMGnQFVfumMs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "MPI широко используется в научных и инженерных вычислениях, где требуется высокая производительность и масштабируемость параллельных вычислений. С его помощью можно создавать параллельные программы для различных приложений, от численного моделирования и анализа данных до расчетов на суперкомпьютерах.\n"
      ],
      "metadata": {
        "id": "ssoM_62iwgHP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Основные понятия**\n",
        "\n",
        "MPI (Message Passing Interface): MPI - это стандарт для обмена сообщениями между процессами в параллельных вычислениях. Он позволяет программистам создавать параллельные приложения, в которых несколько процессов могут обмениваться данными и координировать свою работу.\n",
        "\n",
        "Коммуникаторы MPI определяют группу процессов, между которыми можно обмениваться сообщениями. Существуют встроенные коммуникаторы, такие как MPI.COMM_WORLD, который включает все процессы, запущенные в рамках одной программы.\n",
        "\n",
        "Обмен сообщениями: MPI предоставляет функции для отправки и приема сообщений между процессами. Это основной механизм для обмена данными в параллельных вычислениях.\n",
        "\n",
        "Collective Communication: Кроме отправки и приема индивидуальных сообщений, MPI также предоставляет коллективные операции, такие как MPI_Bcast, MPI_Reduce и MPI_Scatter, которые позволяют одному процессу отправлять данные всем или некоторым другим процессам.\n",
        "\n",
        "Библиотека mpi4py: Для работы с MPI в Python часто используется библиотека mpi4py, которая предоставляет доступ к функциям MPI через интерфейс Python."
      ],
      "metadata": {
        "id": "UP4wv3kSyEfn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Установим mpi4py\n"
      ],
      "metadata": {
        "id": "ZFlsZC1dSbpC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install mpi4py\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-TooMQqiSYfG",
        "outputId": "a68ccac5-4582-402a-dcb8-3efdba557b00"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: mpi4py in /usr/local/lib/python3.10/dist-packages (3.1.6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Проверим установкучерез файл"
      ],
      "metadata": {
        "id": "kQGQ7G260j-a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile test.py\n",
        "from mpi4py import MPI\n",
        "\n",
        "comm = MPI.COMM_WORLD\n",
        "rank = comm.Get_rank()\n",
        "size = comm.Get_size()\n",
        "\n",
        "print(\"Hello! I'm process\", rank, \"out of\", size)\n",
        "\n",
        "MPI.Finalize()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lT9D5aSV75xn",
        "outputId": "27f3bfd0-c700-44ca-df81-0f7f89c9b0ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing test.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Запустим программу\n"
      ],
      "metadata": {
        "id": "RG8Yin0vWvde"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mpiexec -n 4 python test.py\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e9q_jaDsWpuw",
        "outputId": "b224ec8a-5de2-42e3-bc96-573f3fba6699"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------------------------------\n",
            "mpiexec has detected an attempt to run as root.\n",
            "\n",
            "Running as root is *strongly* discouraged as any mistake (e.g., in\n",
            "defining TMPDIR) or bug can result in catastrophic damage to the OS\n",
            "file system, leaving your system in an unusable state.\n",
            "\n",
            "We strongly suggest that you run mpiexec as a non-root user.\n",
            "\n",
            "You can override this protection by adding the --allow-run-as-root option\n",
            "to the cmd line or by setting two environment variables in the following way:\n",
            "the variable OMPI_ALLOW_RUN_AS_ROOT=1 to indicate the desire to override this\n",
            "protection, and OMPI_ALLOW_RUN_AS_ROOT_CONFIRM=1 to confirm the choice and\n",
            "add one more layer of certainty that you want to do so.\n",
            "We reiterate our advice against doing so - please proceed at your own risk.\n",
            "--------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Пример: Параллельное выполнение простых задач**\n",
        "\n",
        "\n",
        "В этом примере создается несколько процессов MPI, каждый из которых выполняет свою задачу. Главный процесс с рангом 0 выполняет определенную задачу, в то время как остальные процессы (рабочие) выполняют другие задачи. Такое параллельное выполнение позволяет ускорить выполнение программы, разделяя работу между несколькими процессами."
      ],
      "metadata": {
        "id": "voVgrydG8BZa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from mpi4py import MPI\n",
        "\n",
        "# Инициализация MPI\n",
        "comm = MPI.COMM_WORLD\n",
        "rank = comm.Get_rank()\n",
        "size = comm.Get_size()\n",
        "\n",
        "# Пример параллельного выполнения простых задач\n",
        "if rank == 0:\n",
        "    print(\"Master process is running\")\n",
        "    # Задача для главного процесса\n",
        "else:\n",
        "    print(f\"Worker process {rank} is running\")\n",
        "    # Задача для рабочих процессов\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n2tWw3_16gXW",
        "outputId": "1087c750-ea17-4340-fb21-f3f24ceca90d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Master process is running\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Пример : Использования сборки данных с нескольких процессои**\n",
        "\n",
        "Этот пример создает несколько процессов MPI, каждый из которых генерирует данные на основе своего ранга. Затем собранные данные с каждого процесса собираются вместе с помощью функции gather на процессе с рангом 0."
      ],
      "metadata": {
        "id": "JBTEGJoFmboZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from mpi4py import MPI\n",
        "\n",
        "comm = MPI.COMM_WORLD\n",
        "rank = comm.Get_rank()\n",
        "\n",
        "data = rank + 1\n",
        "gathered_data = comm.gather(data, root=0)\n",
        "\n",
        "if rank == 0:\n",
        "    print(f\"Process {rank} gathered data: {gathered_data}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oTahN6iQmcL1",
        "outputId": "08a38247-d6fc-40b9-9846-b3b53a44afaf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Process 0 gathered data: [1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Пример: Использование библиотеки NumPy для генерации данных**\\\n",
        "Этот пример демонстрирует использование функции `bcast` для распространения данных от процесса с рангом 0 ко всем остальным процессам в коммуникаторе MPI. Сначала процесс с рангом 0 генерирует массив данных при помощи библиотеки NumPy. Затем используется функция `bcast`, чтобы передать этот массив от процесса с рангом 0 ко всем остальным процессам. Если процесс имеет ранг 0, он отправляет данные, иначе он просто принимает переданные данные. Таким образом, все процессы получают одинаковые данные, сгенерированные процессом с рангом 0.\n"
      ],
      "metadata": {
        "id": "4N-5NMIPrEgB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from mpi4py import MPI\n",
        "import numpy as np\n",
        "\n",
        "# Инициализация MPI\n",
        "comm = MPI.COMM_WORLD\n",
        "rank = comm.Get_rank()\n",
        "size = comm.Get_size()\n",
        "\n",
        "# Генерация данных с использованием библиотеки NumPy\n",
        "if rank == 0:\n",
        "    data = np.random.rand(10)\n",
        "    print(\"Generated data:\", data)\n",
        "\n",
        "# Рассылка данных от процесса с рангом 0 ко всем остальным процессам\n",
        "data = comm.bcast(data if rank == 0 else None, root=0)\n",
        "print(\"Rank\", rank, \"received data:\", data)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ynMzvF8rCJy",
        "outputId": "dd687c47-e861-4ea2-88f4-cc3a4a154cfe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated data: [0.90032337 0.34666114 0.98828865 0.9728472  0.76280179 0.11558258\n",
            " 0.74625381 0.92558524 0.83581736 0.03676646]\n",
            "Rank 0 received data: [0.90032337 0.34666114 0.98828865 0.9728472  0.76280179 0.11558258\n",
            " 0.74625381 0.92558524 0.83581736 0.03676646]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " **Пример: Использование функции scatter и библиотеки mpi4y**\n",
        "\n",
        "Пример демонстрирует использование функции scatter в библиотеке mpi4py для распределения данных между процессами MPI. В начале программы инициализируется MPI коммуникатор и определяется ранг текущего процесса. Затем процесс с рангом 0 создает массив numpy из 10 элементов, заполненных нулями, который будет распределяться между процессами. Для этого массива создается список с одним элементом - самим массивом. Затем используется функция scatter, чтобы каждый процесс получил одну часть данных. Каждый процесс выводит полученную часть данных и свой ранг. Таким образом, функция scatter позволяет распределить массив данных между процессами MPI для параллельной обработки"
      ],
      "metadata": {
        "id": "g9B-NqVYpx6_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from mpi4py import MPI\n",
        "import numpy as np\n",
        "\n",
        "comm = MPI.COMM_WORLD\n",
        "rank = comm.Get_rank()\n",
        "\n",
        "# Процесс с рангом 0 создает исходные данные\n",
        "if rank == 0:\n",
        "    data = np.zeros(10)  # Создаем массив numpy с 10 элементами\n",
        "else:\n",
        "    data = None\n",
        "\n",
        "# Распределение данных между процессами с использованием функции scatter\n",
        "chunk = comm.scatter([data], root=0)  # Передаем список с одним элементом - массивом data\n",
        "print(\"Rank\", rank, \"received data chunk:\", chunk)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rfUGdX4ipyQq",
        "outputId": "897b8cca-6aad-476b-b39c-4d3967b86398"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rank 0 received data chunk: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Упражнение 1: Использование трансляции\n",
        "\n",
        "**Задание**: Напишите программу, где процесс 0 передаёт массив всем остальным процессам с использованием функции `bcast`.\n",
        "\n"
      ],
      "metadata": {
        "id": "l7030oPRuUGS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from mpi4py import MPI\n",
        "\n",
        "comm = MPI.COMM_WORLD\n",
        "rank = comm.Get_rank()\n",
        "\n",
        "if rank == 0:\n",
        "    data = [1, 2, 3, 4, 5]\n",
        "else:\n",
        "    data = None\n",
        "\n",
        "data = comm.bcast(data, root=0)\n",
        "print(\"Process\", rank, \"received data:\", data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lAmuR4FAw2wE",
        "outputId": "8d63737f-a20e-46d1-8dc4-4a526e7c6b2a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Process 0 received data: [1, 2, 3, 4, 5]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Упражнение 2: Сбор данных\n",
        "\n",
        "**Задание**: Каждый процесс генерирует случайное число. Используйте `gather` для сбора всех чисел в одном массиве в процессе 0."
      ],
      "metadata": {
        "id": "GBbc_4j6wnKE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from mpi4py import MPI\n",
        "\n",
        "comm = MPI.COMM_WORLD\n",
        "rank = comm.Get_rank()\n",
        "\n",
        "# Генерация случайного числа\n",
        "rand_num = np.random.rand()\n",
        "\n",
        "# Собираем все числа в процессе 0\n",
        "all_rand_nums = comm.gather(rand_num, root=0)\n",
        "\n",
        "if rank == 0:\n",
        "    print(\"Random numbers gathered at root:\", all_rand_nums)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5aemiz0BwNLd",
        "outputId": "dcef32ff-90b3-4c90-a554-3c2c25fb6781"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random numbers gathered at root: [0.2882202681921189]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Упражнение 3: Рассылка суммы\n",
        "\n",
        "**Задание**: Процесс 0 собирает числа от всех процессов, вычисляет их сумму и рассылает эту сумму всем процессам."
      ],
      "metadata": {
        "id": "hwsfcQY9wFNm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from mpi4py import MPI\n",
        "\n",
        "comm = MPI.COMM_WORLD\n",
        "rank = comm.Get_rank()\n",
        "\n",
        "# Все процессы генерируют случайное число\n",
        "rand_num = np.random.rand()\n",
        "print(\"Process\", rank, \"generated number:\", rand_num)\n",
        "\n",
        "# Собираем все числа в процессе 0\n",
        "all_rand_nums = comm.gather(rand_num, root=0)\n",
        "\n",
        "if rank == 0:\n",
        "    total_sum = sum(all_rand_nums)\n",
        "    print(\"Total sum calculated at root:\", total_sum)\n",
        "else:\n",
        "    total_sum = None\n",
        "\n",
        "# Рассылаем сумму всем процессам\n",
        "total_sum = comm.bcast(total_sum, root=0)\n",
        "print(\"Process\", rank, \"received total sum:\", total_sum)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xiDxRy1Tv85A",
        "outputId": "de055378-1fc7-47d0-fcce-72521f17e3cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Process 0 generated number: 0.4290660787289786\n",
            "Total sum calculated at root: 0.4290660787289786\n",
            "Process 0 received total sum: 0.4290660787289786\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Упражнение 4: Параллельное вычисление числа Пи методом Монте-Карло\n",
        "\n",
        "**Задание**: Используйте метод Монте-Карло для оценки числа Пи. Каждый процесс генерирует случайные точки и определяет, сколько из них попадает внутрь круга. Используйте `reduce` для сбора и суммирования результатов, а затем вычислите и распечатайте приближенное значение числа Пи на процессе с рангом 0.\n"
      ],
      "metadata": {
        "id": "0GPv0doKwQ5H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from mpi4py import MPI\n",
        "import numpy as np\n",
        "\n",
        "comm = MPI.COMM_WORLD\n",
        "rank = comm.Get_rank()\n",
        "size = comm.Get_size()\n",
        "\n",
        "# Количество точек для генерации в каждом процессе\n",
        "points_per_process = 1000000\n",
        "\n",
        "# Генерация случайных точек и подсчёт количества точек внутри круга\n",
        "count = 0\n",
        "for _ in range(points_per_process):\n",
        "    x, y = np.random.rand(2)\n",
        "    if x**2 + y**2 <= 1:\n",
        "        count += 1\n",
        "\n",
        "# Собираем все счетчики в root процессе\n",
        "total_count = comm.reduce(count, op=MPI.SUM, root=0)\n",
        "\n",
        "# Вычисляем Пи на процессе 0\n",
        "if rank == 0:\n",
        "    pi_estimate = (4 * total_count) / (size * points_per_process)\n",
        "    print(\"Estimated Pi value:\", pi_estimate)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ulWjrFXYxjew",
        "outputId": "b1da2be1-77d1-490a-8efe-f81ef04f3203"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Estimated Pi value: 3.139368\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Упражнение 5: Параллельное вычисление среднего\n",
        "\n",
        "**Задание**: Напишите программу, где каждый процесс генерирует список случайных чисел. Используйте `reduce` для нахождения общей суммы всех чисел, а затем вычислите среднее значение на процессе с рангом 0."
      ],
      "metadata": {
        "id": "9bKRX9iQxYOt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from mpi4py import MPI\n",
        "import numpy as np\n",
        "\n",
        "comm = MPI.COMM_WORLD\n",
        "rank = comm.Get_rank()\n",
        "size = comm.Get_size()\n",
        "\n",
        "# Каждый процесс генерирует список из 10 случайных чисел\n",
        "local_data = np.random.rand(10)\n",
        "print(\"Process\", rank, \"generated data:\", local_data)\n",
        "\n",
        "# Используем MPI reduce для суммирования всех списков в процессе 0\n",
        "total_data = comm.reduce(np.sum(local_data), op=MPI.SUM, root=0)\n",
        "\n",
        "if rank == 0:\n",
        "    # Вычисляем среднее значение\n",
        "    mean_value = total_data / (size * 10)\n",
        "    print(\"Mean value calculated on root process:\", mean_value)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5P8jPYJexQat",
        "outputId": "cf6f0076-fa1c-446d-bd6b-221f0cd004f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Process 0 generated data: [0.27075899 0.05401882 0.31366267 0.09037693 0.97128617 0.78278633\n",
            " 0.43195784 0.32104946 0.22368985 0.73551486]\n",
            "Mean value calculated on root process: 0.41951019265611755\n"
          ]
        }
      ]
    }
  ]
}